{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Recognition using Transfer Learning with MobileNetV2\n",
    "\n",
    "This notebook demonstrates how to build an American Sign Language (ASL) recognition system using transfer learning. We use MobileNetV2 as a base model to perform feature extraction, and then add a new classification head. The notebook is divided into several sections:\n",
    "- **Imports & Setup:** Load necessary libraries.\n",
    "- **Model Building:** Define a transfer learning model with data augmentation.\n",
    "- **Training:** Load the ASL dataset, train the model, and visualize performance (accuracy, loss, confusion matrix, and per-class accuracy).\n",
    "- **Real-Time Detection:** Use MediaPipe and OpenCV to capture webcam video, detect hands, and predict ASL signs in real time.\n",
    "\n",
    "Follow the cells sequentially to build, train, and test the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Essential Libraries\n",
    "\n",
    "This code snippet imports several important libraries that will be used throughout the script:\n",
    "\n",
    "- sys and os: Provide system-specific functionalities, such as interacting with the operating system and handling file paths.\n",
    "- cv2 (OpenCV): A powerful library for image and video processing, used for real-time video capture and manipulation.\n",
    "- numpy: A fundamental package for numerical computing in Python, mainly used for handling arrays and performing mathematical operations.\n",
    "- tensorflow: A deep learning framework for loading, training, and deploying machine learning models.\n",
    "- matplotlib.pyplot: A visualization library for generating plots, useful for displaying data such as training accuracy and loss curves.\n",
    "- language_tool_python: A library for grammar and spell-checking, suggesting that the script may include some text processing or correction functionalities.\n",
    "- mediapipe: A library designed for real-time computer vision tasks, such as hand tracking and gesture recognition, commonly used in applications involving sign language detection.\n",
    "\n",
    "This combination of imports indicates that the script will likely perform deep learning-based image processing and real-time sign language recognition, potentially using a neural network trained with TensorFlow and integrating OpenCV for video capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import language_tool_python\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for Training\n",
    "\n",
    "This section of the code sets up key parameters for training a deep learning model on an American Sign Language (ASL) dataset.\n",
    "\n",
    "#### Parameter Definitions\n",
    "- data_dir = \"asl_dataset\": Specifies the directory where the dataset of ASL images is stored.\n",
    "- img_size = (224, 224): Defines the target image size for resizing, ensuring compatibility with MobileNetV2, which expects 224x224 images.\n",
    "- batch_size = 32: Determines the number of images processed in each training step.\n",
    "- val_split = 0.2: Allocates 20% of the dataset for validation, leaving 80% for training.\n",
    "- seed = 123: Ensures reproducibility by setting a fixed seed for dataset shuffling.\n",
    "- epochs = 10: Specifies the number of times the model will train over the dataset.\n",
    "- model_save_path = \"asl_model_transfer.keras\": Defines where the trained model will be saved.\n",
    "- class_names_save_path = \"class_names.txt\": Sets the path for saving the class labels.\n",
    "\n",
    "#### Loading the Dataset\n",
    "The dataset is loaded from the asl_dataset directory using TensorFlow's image_dataset_from_directory function. The dataset is split into training and validation sets:\n",
    "- Training Dataset (train_ds): Contains 80% of the images, used for training the model.\n",
    "- Validation Dataset (val_ds): Contains 20% of the images, used for evaluating the model’s performance.\n",
    "- The same seed value ensures that the training-validation split remains consistent across multiple runs.\n",
    "\n",
    "This setup prepares the dataset for efficient training and validation, ensuring that images are properly resized, batched, and shuffled before being fed into the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2771 files belonging to 36 classes.\n",
      "Using 2217 files for training.\n",
      "Found 2771 files belonging to 36 classes.\n",
      "Using 554 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for training\n",
    "data_dir = \"asl_dataset\"\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "val_split = 0.2\n",
    "seed = 123\n",
    "epochs = 10\n",
    "model_save_path = \"asl_model_transfer.keras\"\n",
    "class_names_save_path = \"class_names.txt\"\n",
    "\n",
    "# Load dataset with an 80/20 split\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=val_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=val_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Transfer Learning Model with MobileNetV2\n",
    "\n",
    "This section of the code is responsible for extracting class labels, saving them for future reference, and constructing a deep learning model using MobileNetV2 as a feature extractor.\n",
    "\n",
    "---\n",
    "\n",
    "### Extracting and Saving Class Names\n",
    "- train_ds.class_names: Retrieves the list of class labels from the training dataset.\n",
    "- num_classes = len(class_names): Counts the total number of unique classes in the dataset.\n",
    "- The class names are printed and then saved to a file (class_names.txt). This ensures that the model's predicted labels can be mapped back to meaningful class names during inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Building the Model with Transfer Learning\n",
    "This script leverages MobileNetV2, a lightweight and pre-trained convolutional neural network (CNN), to speed up training while improving accuracy.\n",
    "\n",
    "#### 1. Loading MobileNetV2 as the Base Model\n",
    "- include_top=False: Excludes the top fully connected layers, allowing us to attach a custom classifier.\n",
    "- weights='imagenet': Uses pre-trained ImageNet weights, enabling the model to leverage pre-learned features.\n",
    "- input_shape=(224, 224, 3): Defines the expected input image size.\n",
    "- base_model.trainable = False: Freezes the base model layers to retain their pre-learned features, preventing unnecessary modifications during training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Implementing Data Augmentation\n",
    "A series of augmentation layers are applied to improve generalization:\n",
    "- RandomFlip(\"horizontal\"): Randomly flips images horizontally.\n",
    "- RandomRotation(0.1): Introduces slight rotations to improve robustness.\n",
    "- RandomZoom(0.1): Randomly zooms into images to simulate variations in distance.\n",
    "- RandomContrast(0.1): Adjusts contrast to improve model adaptability to different lighting conditions.\n",
    "\n",
    "These augmentations help prevent overfitting by making the model invariant to minor variations in input images.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Constructing the Custom Classification Head\n",
    "- tf.keras.layers.Input(shape=img_size + (3,)): Defines the input layer for the model.\n",
    "- Data Augmentation and Preprocessing: Images are augmented and preprocessed using MobileNetV2’s preprocessing function.\n",
    "- Feature Extraction: Images pass through the MobileNetV2 base model, which extracts meaningful high-level features.\n",
    "- GlobalAveragePooling2D(): Converts feature maps into a single vector per image.\n",
    "- Dropout(0.2): Adds a dropout layer to reduce overfitting by randomly deactivating neurons during training.\n",
    "- Dense(num_classes, activation='softmax'): Outputs class probabilities using a softmax activation function, enabling multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Compiling and Summarizing the Model\n",
    "The model is compiled using:\n",
    "- optimizer='adam': Uses the Adam optimizer, which adapts learning rates dynamically.\n",
    "- loss='sparse_categorical_crossentropy': A loss function suitable for multi-class classification when class labels are encoded as integers.\n",
    "- metrics=['accuracy']: Tracks accuracy during training.\n",
    "\n",
    "Finally, model.summary() prints a summary of the model’s architecture, showing the number of layers, parameters, and output shapes.\n",
    "\n",
    "This setup enables efficient transfer learning, allowing the model to leverage MobileNetV2’s powerful pre-trained features while learning a new classifier tailored to American Sign Language (ASL) recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">46,116</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │        \u001b[38;5;34m46,116\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304,100</span> (8.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,304,100\u001b[0m (8.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,116</span> (180.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,116\u001b[0m (180.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get and save class names\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"Class Names:\", class_names)\n",
    "with open(class_names_save_path, \"w\") as f:\n",
    "    for name in class_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# ----- Build Transfer-Learning Model using MobileNetV2 -----\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=img_size + (3,),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze base model\n",
    "\n",
    "# Data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=img_size + (3,))\n",
    "x = data_augmentation(inputs)  # apply augmentation\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "x = base_model(x, training=False)  # pass through base model\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "This section of the code initiates the training process for the transfer learning model using MobileNetV2.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Model Training with fit()\n",
    "The model.fit() function is used to train the deep learning model. It takes the following parameters:\n",
    "- train_ds: The training dataset, which consists of labeled images prepared earlier.\n",
    "- validation_data=val_ds: The validation dataset, used to evaluate the model's performance after each epoch.\n",
    "- epochs=epochs: The number of times the entire dataset will be passed through the model during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Happens During Training?\n",
    "- The model learns patterns in the training data by adjusting its internal weights through backpropagation and gradient descent.\n",
    "- The loss function (sparse_categorical_crossentropy) measures the difference between predicted and actual labels.\n",
    "- The Adam optimizer updates the model's weights to minimize the loss.\n",
    "- The model evaluates itself on the validation set after each epoch to monitor its performance and detect potential overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Output of Training\n",
    "- The history object stores the training progress, including:\n",
    "  - Training and validation accuracy\n",
    "  - Training and validation loss\n",
    "- This information can be used later to visualize learning curves and analyze model performance.\n",
    "\n",
    "Once training is complete, the model will be ready for evaluation and real-time sign language recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 592ms/step - accuracy: 0.1310 - loss: 3.3669 - val_accuracy: 0.3989 - val_loss: 2.0792\n",
      "Epoch 2/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 595ms/step - accuracy: 0.4605 - loss: 1.9381 - val_accuracy: 0.6245 - val_loss: 1.3981\n",
      "Epoch 3/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 606ms/step - accuracy: 0.6394 - loss: 1.3894 - val_accuracy: 0.6986 - val_loss: 1.1054\n",
      "Epoch 4/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 612ms/step - accuracy: 0.6897 - loss: 1.1549 - val_accuracy: 0.7184 - val_loss: 0.9555\n",
      "Epoch 5/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 544ms/step - accuracy: 0.7453 - loss: 0.9736 - val_accuracy: 0.7690 - val_loss: 0.8215\n",
      "Epoch 6/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 546ms/step - accuracy: 0.7916 - loss: 0.8192 - val_accuracy: 0.7798 - val_loss: 0.7544\n",
      "Epoch 7/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.7939 - loss: 0.7705 - val_accuracy: 0.7834 - val_loss: 0.7065\n",
      "Epoch 8/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 507ms/step - accuracy: 0.8177 - loss: 0.6879 - val_accuracy: 0.8069 - val_loss: 0.6392\n",
      "Epoch 9/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 505ms/step - accuracy: 0.8327 - loss: 0.6414 - val_accuracy: 0.7996 - val_loss: 0.6030\n",
      "Epoch 10/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 547ms/step - accuracy: 0.8365 - loss: 0.6015 - val_accuracy: 0.8213 - val_loss: 0.5828\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating and Visualizing Model Performance\n",
    "\n",
    "After training the model, this section evaluates its performance using various metrics and visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Plotting Training and Validation Curves\n",
    "The training progress is visualized by plotting:\n",
    "- Accuracy (acc) vs. Validation Accuracy (val_acc): Helps determine whether the model is learning effectively.\n",
    "- Loss (loss) vs. Validation Loss (val_loss): Shows how well the model is minimizing errors over time.\n",
    "\n",
    "Key Insights:\n",
    "- A large gap between training and validation accuracy may indicate overfitting.\n",
    "- If both loss curves are decreasing, the model is likely improving with each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Generating Predictions for Model Evaluation\n",
    "- The model predicts labels for the validation dataset.\n",
    "- Predictions are stored in all_preds, while true labels are stored in all_labels.\n",
    "- These values are converted into NumPy arrays for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Confusion Matrix Visualization\n",
    "- A confusion matrix is created to analyze how well the model distinguishes between different classes.\n",
    "- Each row represents the actual class, while each column represents the predicted class.\n",
    "- The diagonal values indicate correct classifications, while off-diagonal values show misclassifications.\n",
    "- Darker blue shades represent higher values, indicating better classification performance.\n",
    "\n",
    "Key Observations:\n",
    "- If most values lie along the diagonal, the model has high accuracy.\n",
    "- If there are many misclassified cases, adjustments such as more data augmentation or hyperparameter tuning may be needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Per-Class Accuracy Bar Chart\n",
    "- Computes the accuracy for each individual class.\n",
    "- Helps identify which classes are well-learned and which might need improvement.\n",
    "- The model might struggle with certain signs due to similarities between hand gestures.\n",
    "\n",
    "Key Insights:\n",
    "- A low accuracy in some classes may indicate class imbalance or the need for more training data.\n",
    "- If certain classes have consistently low accuracy, additional data augmentation may help.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Final Model Evaluation and Saving\n",
    "- The model is evaluated on the validation dataset, providing final values for loss and accuracy.\n",
    "- The trained model is saved as a .keras file, allowing it to be reused without retraining.\n",
    "\n",
    "This comprehensive evaluation process ensures that the model's performance is well-understood before deploying it for real-time sign language recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 530ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 446ms/step - accuracy: 0.8218 - loss: 0.5646\n",
      "Validation Loss: 0.5828\n",
      "Validation Accuracy: 0.8213\n",
      "Model saved to asl_model_transfer.keras\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# ----- Plot Training Curves -----\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# ----- Additional Evaluation Graphs -----\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    all_preds.extend(np.argmax(preds, axis=1))\n",
    "    all_labels.extend(labels.numpy())\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = tf.math.confusion_matrix(all_labels, all_preds).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-Class Accuracy Bar Chart\n",
    "per_class_accuracy = []\n",
    "for i in range(len(class_names)):\n",
    "    class_total = np.sum(cm[i, :])\n",
    "    acc_val = cm[i, i] / class_total if class_total > 0 else 0\n",
    "    per_class_accuracy.append(acc_val)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_names, per_class_accuracy, color='green')\n",
    "plt.title(\"Per-Class Accuracy\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0, 1])\n",
    "for i, v in enumerate(per_class_accuracy):\n",
    "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and save the model\n",
    "val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time American Sign Language (ASL) Detection Using a Trained Model\n",
    "\n",
    "This section of the code loads a pre-trained model and uses a webcam to detect hand gestures in real-time, identifying sign language gestures with MobileNetV2 and MediaPipe.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Loading the Pre-Trained Model and Class Names\n",
    "- model = tf.keras.models.load_model(model_path): Loads the saved ASL classification model from the file asl_model_transfer.keras.\n",
    "- class_names: Reads the class names from class_names.txt to map numerical predictions to sign labels.\n",
    "- The model will use these class names to label detected hand signs.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Initializing the Webcam for Real-Time Capture\n",
    "- cv2.VideoCapture(0): Opens the default webcam for capturing live video.\n",
    "- If the webcam fails to open, an error message is displayed, and the script exits.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Using MediaPipe for Hand Detection\n",
    "- mp.solutions.hands.Hands(): Initializes MediaPipe's hand tracking solution.\n",
    "  - static_image_mode=False: Optimized for real-time video (not static images).\n",
    "  - max_num_hands=1: Limits detection to one hand at a time.\n",
    "  - min_detection_confidence=0.5: The model detects a hand only if confidence is above 50%.\n",
    "  - min_tracking_confidence=0.5: Ensures smooth tracking of hand landmarks.\n",
    "- The detected hand landmarks are drawn on the webcam feed using mp_drawing.draw_landmarks().\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Hand Gesture Recognition and Preprocessing\n",
    "- The detected hand bounding box is extracted from the frame.\n",
    "- The cropped hand image is resized to 224x224 pixels, the input size required by MobileNetV2.\n",
    "- The image is normalized using MobileNetV2’s preprocessing function to match the format used during training.\n",
    "- The preprocessed image is passed into the model for prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Making Predictions and Displaying Results\n",
    "- The model predicts class probabilities for the captured hand sign.\n",
    "- Confidence Thresholding (threshold=0.8):\n",
    "  - If confidence is above 80%, the detected class is displayed.\n",
    "  - If confidence is low, the detection is marked as uncertain.\n",
    "- The top three predictions (highest probability classes) are printed for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Overlaying Results on the Video Feed\n",
    "- The detected sign label and confidence score are displayed on the webcam feed.\n",
    "- A buffer (text_buffer) stores detected words, allowing real-time sign language sentence formation.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Ending the Detection\n",
    "- The detection runs in a loop until the user presses q.\n",
    "- The webcam is released and the OpenCV window is closed.\n",
    "- The final text buffer is printed, showing the sequence of detected signs.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "This real-time ASL recognition system:\n",
    "- Detects hands using MediaPipe  \n",
    "- Extracts and preprocesses hand images  \n",
    "- Uses a MobileNetV2 model to classify hand gestures  \n",
    "- Displays real-time results with OpenCV  \n",
    "- Saves detected signs in a text buffer for potential sentence formation  \n",
    "\n",
    "This setup allows users to interactively recognize and classify sign language gestures in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Testing (Real-Time Detection) Mode ------------------\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"asl_model_transfer.keras\"\n",
    "class_names_path = \"class_names.txt\"\n",
    "threshold = 0.8\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Load model and class names\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "with open(class_names_path, \"r\") as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "print(\"Loaded class names:\", class_names)\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    sys.exit()\n",
    "\n",
    "text_buffer = \"\"\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        detection_text = \"Detected: ???\"\n",
    "        hand_found = False\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_found = True\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "                x_min, x_max = w, 0\n",
    "                y_min, y_max = h, 0\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if y > y_max: y_max = y\n",
    "                margin = 20\n",
    "                x_min = max(0, x_min - margin)\n",
    "                y_min = max(0, y_min - margin)\n",
    "                x_max = min(w, x_max + margin)\n",
    "                y_max = min(h, y_max + margin)\n",
    "                hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "                if hand_crop.size > 0:\n",
    "                    hand_crop_resized = cv2.resize(hand_crop, img_size)\n",
    "                    hand_crop_resized = hand_crop_resized.astype(\"float32\")\n",
    "                    hand_crop_resized = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
    "                        hand_crop_resized\n",
    "                    )\n",
    "                    hand_crop_resized = np.expand_dims(hand_crop_resized, axis=0)\n",
    "                    preds = model.predict(hand_crop_resized)[0]\n",
    "                    conf = np.max(preds)\n",
    "                    pred_class = np.argmax(preds)\n",
    "                    \n",
    "                    # Debug: print top 3 predictions\n",
    "                    sorted_idx = np.argsort(preds)[::-1]\n",
    "                    top3 = sorted_idx[:3]\n",
    "                    print(\"Prediction distribution (top 3):\")\n",
    "                    for i in top3:\n",
    "                        print(f\"  {class_names[i]}: {preds[i]*100:.2f}%\")\n",
    "                    print(\"-------------------------\")\n",
    "                    \n",
    "                    if conf > threshold:\n",
    "                        sign_label = class_names[pred_class]\n",
    "                        detection_text = f\"Detected: {sign_label} ({conf:.2f})\"\n",
    "                        text_buffer += sign_label\n",
    "                    else:\n",
    "                        sign_label = class_names[pred_class]\n",
    "                        detection_text = f\"Low confidence: {sign_label} ({conf:.2f})\"\n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            detection_text,\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0) if hand_found else (0, 0, 255),\n",
    "            2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Buffer: {text_buffer}\",\n",
    "            (10, 70),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2\n",
    "        )\n",
    "        cv2.imshow(\"ASL Real-Time with MobileNet\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Final text buffer:\", text_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
